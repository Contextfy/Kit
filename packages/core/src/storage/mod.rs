use crate::parser::{extract_summary, ParsedDoc};
use anyhow::{Context, Result};
// use lancedb::connect;
// use arrow::array::{StringArray, StringBuilder};
// use arrow::datatypes::{DataType, Field, Schema as ArrowSchema};
// use arrow::record_batch::RecordBatch;
use serde::{Deserialize, Serialize};
use std::path::{Path, PathBuf};
use tokio::fs;
use uuid::Uuid;

/// çŸ¥è¯†åº“ä¸­çš„ä¸€æ¡è®°å½•
///
/// # å­—æ®µ
///
/// * `id` - è®°å½•çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆUUIDï¼‰
/// * `title` - è®°å½•æ ‡é¢˜ï¼ˆå¯¹äºåˆ‡ç‰‡æ–‡æ¡£ï¼Œè¿™æ˜¯ H2 æ ‡é¢˜ï¼‰
/// * `parent_doc_title` - çˆ¶æ–‡æ¡£çš„æ ‡é¢˜ï¼ˆH1 æ ‡é¢˜æˆ–æ–‡ä»¶åï¼‰
/// * `summary` - å†…å®¹æ‘˜è¦ï¼ˆå‰ 200 ä¸ªå­—ç¬¦ï¼‰
/// * `content` - å®Œæ•´å†…å®¹
/// * `source_path` - åŸå§‹æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè¿½æº¯æºæ–‡ä»¶
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct KnowledgeRecord {
    pub id: String,
    pub title: String,
    pub parent_doc_title: String,
    pub summary: String,
    pub content: String,
    pub source_path: String, // æ–°å¢å­—æ®µï¼šè®°å½•åŸå§‹æ–‡ä»¶è·¯å¾„
}

pub struct KnowledgeStore {
    data_dir: String,
}

impl KnowledgeStore {
    pub async fn new(data_dir: &str) -> Result<Self> {
        fs::create_dir_all(data_dir).await?;

        // å¯åŠ¨æ¢å¤ï¼šæ¸…ç†ä¸Šæ¬¡å´©æºƒé—ç•™çš„ä¸´æ—¶ç›®å½•
        Self::cleanup_orphaned_temp_dirs(data_dir).await;

        Ok(KnowledgeStore {
            data_dir: data_dir.to_string(),
        })
    }

    /// æ¸…ç†å­¤å„¿ä¸´æ—¶ç›®å½•ï¼ˆå¯åŠ¨æ¢å¤ï¼‰
    ///
    /// å¦‚æœç¨‹åºåœ¨å†™å…¥è¿‡ç¨‹ä¸­å´©æºƒï¼Œå¯èƒ½ä¼šé—ç•™ `.temp-{uuid}` ç›®å½•ã€‚
    /// è¿™ä¸ªæ–¹æ³•åœ¨å¯åŠ¨æ—¶æ‰«æå¹¶åˆ é™¤è¿™äº›ç›®å½•ã€‚
    async fn cleanup_orphaned_temp_dirs(data_dir: &str) {
        let mut entries = match fs::read_dir(data_dir).await {
            Ok(entries) => entries,
            Err(_) => return,
        };

        while let Ok(Some(entry)) = entries.next_entry().await {
            let name = entry.file_name();

            // æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶ç›®å½•ï¼ˆä»¥ .temp- å¼€å¤´ï¼‰
            let name_str = match name.to_str() {
                Some(s) => s,
                None => continue,
            };

            if name_str.starts_with(".temp-") && entry.path().is_dir() {
                eprintln!("Cleaning up orphaned temp directory: {}", name_str);
                let _ = fs::remove_dir_all(entry.path()).await;
            }
        }
    }

    /// åˆ›å»ºä¸´æ—¶å†™å…¥ç›®å½•
    ///
    /// ç”¨äºå®ç°åŸå­æ€§å†™å…¥ï¼šæ‰€æœ‰æ–‡ä»¶å…ˆå†™å…¥ä¸´æ—¶ç›®å½•ï¼ŒæˆåŠŸåæ‰¹é‡ç§»åŠ¨åˆ°æ­£å¼ç›®å½•ã€‚
    /// å¦‚æœä¸­é€”å¤±è´¥ï¼Œä¸´æ—¶ç›®å½•ä¼šè¢«åˆ é™¤ï¼Œç¡®ä¿ä¸ä¼šç•™ä¸‹"å¹½çµæ•°æ®"ã€‚
    async fn create_temp_dir(&self) -> Result<PathBuf> {
        let temp_dir = Path::new(&self.data_dir).join(format!(".temp-{}", Uuid::new_v4()));
        fs::create_dir_all(&temp_dir).await?;
        Ok(temp_dir)
    }

    /// æ¸…ç†ä¸´æ—¶ç›®å½•ï¼ˆå¤±è´¥æ—¶è°ƒç”¨ï¼‰
    async fn cleanup_temp_dir(temp_dir: &Path) {
        let _ = fs::remove_dir_all(temp_dir).await;
    }

    pub async fn search(&self, query: &str) -> Result<Vec<KnowledgeRecord>> {
        let mut records = Vec::new();
        let mut entries = fs::read_dir(&self.data_dir).await?;

        while let Some(entry) = entries.next_entry().await? {
            let path = entry.path();

            // é˜²å¾¡æ€§æ£€æŸ¥ï¼šè·³è¿‡ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
            if Self::is_temp_file(&path) {
                continue;
            }

            if !path.is_file() {
                continue;
            }

            if path.extension().and_then(|s| s.to_str()) == Some("json") {
                let content = fs::read_to_string(&path).await?;
                if let Ok(record) = serde_json::from_str::<KnowledgeRecord>(&content) {
                    if record.title.to_lowercase().contains(&query.to_lowercase())
                        || record
                            .summary
                            .to_lowercase()
                            .contains(&query.to_lowercase())
                    {
                        records.push(record);
                    }
                }
            }
        }

        Ok(records)
    }

    pub async fn get(&self, id: &str) -> Result<Option<KnowledgeRecord>> {
        let mut entries = fs::read_dir(&self.data_dir).await?;

        while let Some(entry) = entries.next_entry().await? {
            let path = entry.path();

            // é˜²å¾¡æ€§æ£€æŸ¥ï¼šè·³è¿‡ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•
            if Self::is_temp_file(&path) {
                continue;
            }

            if !path.is_file() {
                continue;
            }

            if path.extension().and_then(|s| s.to_str()) == Some("json") {
                let content = fs::read_to_string(&path).await?;
                if let Ok(record) = serde_json::from_str::<KnowledgeRecord>(&content) {
                    if record.id == id {
                        return Ok(Some(record));
                    }
                }
            }
        }

        Ok(None)
    }

    /// æ£€æŸ¥è·¯å¾„æ˜¯å¦æ˜¯ä¸´æ—¶æ–‡ä»¶ï¼ˆé˜²å¾¡æ€§æ£€æŸ¥ï¼‰
    ///
    /// ä¸´æ—¶æ–‡ä»¶/ç›®å½•çš„**åç§°**ä»¥ `.temp-` å¼€å¤´ï¼Œåº”è¯¥åœ¨æ­£å¸¸æ‰«æä¸­è¢«è·³è¿‡ã€‚
    /// åªæ£€æŸ¥è·¯å¾„çš„æœ€åä¸€ä¸ªç»„ä»¶ï¼ˆæ–‡ä»¶åæˆ–ç›®å½•åï¼‰ï¼Œé¿å…è¯¯åˆ¤åŒ…å« `.temp-` çš„çˆ¶ç›®å½•è·¯å¾„ã€‚
    fn is_temp_file(path: &Path) -> bool {
        path.file_name()
            .and_then(|name| name.to_str())
            .map(|name| name.starts_with(".temp-"))
            .unwrap_or(false)
    }

    /// æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼ˆå¸¦å›æ»šæœºåˆ¶çš„åŸå­æ€§å†™å…¥ï¼‰
    ///
    /// # å›æ»šæœºåˆ¶
    ///
    /// 1. **å‡†å¤‡é˜¶æ®µ**ï¼šåˆ›å»ºä¸´æ—¶ç›®å½•ï¼ˆ`.data/.temp-{uuid}`ï¼‰
    /// 2. **å†™å…¥é˜¶æ®µ**ï¼šæ‰€æœ‰åˆ‡ç‰‡å…ˆå†™å…¥ä¸´æ—¶ç›®å½•
    /// 3. **æäº¤é˜¶æ®µ**ï¼šå…¨éƒ¨æˆåŠŸåï¼ŒåŸå­æ€§ç§»åŠ¨æ–‡ä»¶åˆ°æ­£å¼ç›®å½•
    /// 4. **å›æ»šé˜¶æ®µ**ï¼šä»»ä½•å¤±è´¥å‘ç”Ÿæ—¶ï¼Œåˆ é™¤æ•´ä¸ªä¸´æ—¶ç›®å½•å’Œå·²æäº¤çš„æ–‡ä»¶
    ///
    /// è¿™æ ·ç¡®ä¿äº†åŸå­æ€§ï¼šè¦ä¹ˆæ‰€æœ‰åˆ‡ç‰‡éƒ½æˆåŠŸå†™å…¥ï¼Œè¦ä¹ˆéƒ½ä¸å†™å…¥ã€‚
    /// ä¸ä¼šå‡ºç°"éƒ¨åˆ†å†™å…¥"å¯¼è‡´çš„"å¹½çµæ•°æ®"é—®é¢˜ã€‚
    pub async fn add(&self, doc: &ParsedDoc) -> Result<Vec<String>> {
        let mut ids = Vec::new();

        if doc.sections.is_empty() {
            // å›é€€é€»è¾‘ï¼šå¦‚æœæ–‡æ¡£æ²¡æœ‰åˆ‡ç‰‡ï¼Œå°†æ•´ä¸ªæ–‡æ¡£ä½œä¸ºå•æ¡è®°å½•å­˜å‚¨
            // è¿™ç§æƒ…å†µå¯èƒ½å‡ºç°åœ¨ï¼š
            // 1. æ–‡æ¡£æ²¡æœ‰ H2 æ ‡é¢˜
            // 2. æ—§ç‰ˆæœ¬è§£æçš„æ–‡æ¡£ï¼ˆå‘åå…¼å®¹ï¼‰
            //
            // ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ¨¡å¼ç¡®ä¿åŸå­æ€§ï¼šå†™å…¥ä¸´æ—¶æ–‡ä»¶ -> åŸå­é‡å‘½å
            let id = Uuid::new_v4().to_string();
            let record = KnowledgeRecord {
                id: id.clone(),
                title: doc.title.clone(),
                parent_doc_title: doc.title.clone(),
                summary: doc.summary.clone(),
                content: doc.content.clone(),
                source_path: doc.path.clone(),
            };

            // åˆ›å»ºä¸´æ—¶ç›®å½•
            let temp_dir = match self.create_temp_dir().await {
                Ok(dir) => dir,
                Err(e) => {
                    return Err(e).context("Failed to create temporary directory");
                }
            };

            // åºåˆ—åŒ–å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶
            let json = match serde_json::to_string_pretty(&record) {
                Ok(j) => j,
                Err(e) => {
                    Self::cleanup_temp_dir(&temp_dir).await;
                    return Err(e).context("Failed to serialize document");
                }
            };

            let temp_path = temp_dir.join(format!("{}.json", id));
            if let Err(e) = fs::write(&temp_path, json).await {
                Self::cleanup_temp_dir(&temp_dir).await;
                return Err(e).context("Failed to write temporary file");
            }

            // åŸå­æ€§ç§»åŠ¨åˆ°æ­£å¼ç›®å½•
            let final_path = Path::new(&self.data_dir).join(format!("{}.json", id));
            if let Err(e) = fs::rename(&temp_path, &final_path).await {
                Self::cleanup_temp_dir(&temp_dir).await;
                return Err(e).context("Failed to move file to final destination");
            }

            // æ¸…ç†ä¸´æ—¶ç›®å½•
            Self::cleanup_temp_dir(&temp_dir).await;
            ids.push(id);
        } else {
            // æ–°é€»è¾‘ï¼šä¸ºæ¯ä¸ªåˆ‡ç‰‡åˆ›å»ºç‹¬ç«‹çš„è®°å½•ï¼ˆå¸¦å›æ»šæœºåˆ¶ï¼‰
            //
            // ã€é—®é¢˜ã€‘å¦‚æœç¬¬ 5 ä¸ªåˆ‡ç‰‡å†™å…¥å¤±è´¥ï¼ˆæ¯”å¦‚ç£ç›˜æ»¡ï¼‰ï¼Œå‰ 4 ä¸ªåˆ‡ç‰‡å·²ç»ç•™åœ¨é‚£äº†ï¼Œ
            // å˜æˆäº†"å¹½çµæ•°æ®"ï¼Œå¯¼è‡´æ•°æ®ä¸ä¸€è‡´ã€‚
            //
            // ã€è§£å†³æ–¹æ¡ˆã€‘ä½¿ç”¨ä¸´æ—¶ç›®å½• + åŸå­ç§»åŠ¨ + å›æ»šæœºåˆ¶ï¼š
            // 1. æ‰€æœ‰åˆ‡ç‰‡å…ˆå†™å…¥ä¸´æ—¶ç›®å½•
            // 2. å…¨éƒ¨æˆåŠŸåï¼Œæ‰¹é‡ç§»åŠ¨åˆ°æ­£å¼ç›®å½•ï¼ˆè®°å½•å·²ç§»åŠ¨çš„æ–‡ä»¶ï¼‰
            // 3. ä»»ä½•å¤±è´¥å‘ç”Ÿæ—¶ï¼Œåˆ é™¤å·²ç§»åŠ¨çš„æ–‡ä»¶å’Œä¸´æ—¶ç›®å½•
            //
            // è¿™æ ·ç¡®ä¿äº†åŸå­æ€§ï¼šè¦ä¹ˆå…¨éƒ¨æˆåŠŸï¼Œè¦ä¹ˆå…¨éƒ¨å¤±è´¥ã€‚

            // æ­¥éª¤ 1ï¼šåˆ›å»ºä¸´æ—¶ç›®å½•
            let temp_dir = match self.create_temp_dir().await {
                Ok(dir) => dir,
                Err(e) => {
                    return Err(e).context("Failed to create temporary directory");
                }
            };

            // æ­¥éª¤ 2ï¼šåœ¨ä¸´æ—¶ç›®å½•ä¸­å†™å…¥æ‰€æœ‰åˆ‡ç‰‡
            let mut temp_files = Vec::new();

            for slice in &doc.sections {
                let id = Uuid::new_v4().to_string();
                let temp_path = temp_dir.join(format!("{}.json", id));

                // åºåˆ—åŒ–å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶
                let record = KnowledgeRecord {
                    id: id.clone(),
                    title: slice.section_title.clone(),
                    parent_doc_title: slice.parent_doc_title.clone(),
                    summary: extract_summary(&slice.content),
                    content: slice.content.clone(),
                    source_path: doc.path.clone(),
                };

                // åºåˆ—åŒ–å¤±è´¥æ—¶æ¸…ç†ä¸´æ—¶ç›®å½•
                let json = match serde_json::to_string_pretty(&record) {
                    Ok(j) => j,
                    Err(e) => {
                        Self::cleanup_temp_dir(&temp_dir).await;
                        return Err(e).context(format!(
                            "Failed to serialize slice: {}",
                            slice.section_title
                        ));
                    }
                };

                // å¦‚æœå†™å…¥å¤±è´¥ï¼Œæ¸…ç†ä¸´æ—¶ç›®å½•å¹¶è¿”å›é”™è¯¯
                if let Err(e) = fs::write(&temp_path, json).await {
                    Self::cleanup_temp_dir(&temp_dir).await;
                    return Err(e).context(format!(
                        "Failed to write temporary file for slice: {}",
                        slice.section_title
                    ));
                }

                ids.push(id.clone());
                temp_files.push((id, temp_path));
            }

            // æ­¥éª¤ 3ï¼šå…¨éƒ¨æˆåŠŸåï¼Œç§»åŠ¨æ–‡ä»¶åˆ°æ­£å¼ç›®å½•ï¼ˆå¸¦å›æ»šæœºåˆ¶ï¼‰
            let mut committed_files = Vec::new();

            for (id, temp_path) in temp_files {
                let final_path = Path::new(&self.data_dir).join(format!("{}.json", id));

                match fs::rename(&temp_path, &final_path).await {
                    Ok(_) => {
                        committed_files.push(final_path);
                    }
                    Err(e) => {
                        // å›æ»šï¼šåˆ é™¤æ‰€æœ‰å·²ç§»åŠ¨åˆ°æ­£å¼ç›®å½•çš„æ–‡ä»¶
                        for path in &committed_files {
                            let _ = fs::remove_file(path).await;
                        }
                        Self::cleanup_temp_dir(&temp_dir).await;
                        return Err(e)
                            .context(format!("Failed to move file {} to final destination", id))
                            .context("Transaction rolled back: all committed files removed");
                    }
                }
            }

            // æ­¥éª¤ 4ï¼šåˆ é™¤ä¸´æ—¶ç›®å½•ï¼ˆæ­¤æ—¶åº”è¯¥å·²ç»ä¸ºç©ºï¼‰
            Self::cleanup_temp_dir(&temp_dir).await;
        }

        Ok(ids) // è¿”å›æ‰€æœ‰åˆ‡ç‰‡çš„ IDï¼ˆå¦‚æœæœ‰åˆ‡ç‰‡ï¼‰æˆ–å•ä¸ªæ–‡æ¡£ ID
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::parser::SlicedSection;
    use std::fs;

    /// åŸºæœ¬åˆ‡ç‰‡å­˜å‚¨æµ‹è¯•
    #[tokio::test]
    async fn test_add_sliced_doc() {
        // åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
        let temp_dir = tempfile::tempdir().unwrap();
        let store = KnowledgeStore::new(temp_dir.path().to_str().unwrap())
            .await
            .unwrap();

        // æ‰‹åŠ¨æ„é€ åŒ…å« 2 ä¸ªåˆ‡ç‰‡çš„ ParsedDoc
        let doc = ParsedDoc {
            path: "/fake/path.md".to_string(),
            title: "Test Doc".to_string(),
            summary: "Test summary".to_string(),
            content: "Full content".to_string(),
            sections: vec![
                SlicedSection {
                    section_title: "Section 1".to_string(),
                    content: "Content 1".to_string(),
                    parent_doc_title: "Test Doc".to_string(),
                    summary: "Content 1".to_string(),
                },
                SlicedSection {
                    section_title: "Section 2".to_string(),
                    content: "Content 2".to_string(),
                    parent_doc_title: "Test Doc".to_string(),
                    summary: "Content 2".to_string(),
                },
            ],
        };

        // è°ƒç”¨ add()
        let ids = store.add(&doc).await.unwrap();

        // æ–­è¨€ï¼šè¿”å› 2 ä¸ª ID
        assert_eq!(ids.len(), 2);

        // æ–­è¨€ï¼šå­˜å‚¨ç›®å½•ä¸­æœ‰ 2 ä¸ª JSON æ–‡ä»¶
        let json_files: Vec<_> = fs::read_dir(temp_dir.path())
            .unwrap()
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("json"))
            .collect();
        assert_eq!(json_files.len(), 2);

        // æ–­è¨€ï¼šæ¯ä¸ªè®°å½•éƒ½æœ‰æ­£ç¡®çš„ source_path
        for json_file in json_files {
            let content = fs::read_to_string(json_file.path()).unwrap();
            let record: KnowledgeRecord = serde_json::from_str(&content).unwrap();
            assert_eq!(record.source_path, "/fake/path.md");
        }
    }

    /// ç©ºåˆ‡ç‰‡å›é€€æµ‹è¯•
    #[tokio::test]
    async fn test_add_empty_sections() {
        // åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
        let temp_dir = tempfile::tempdir().unwrap();
        let store = KnowledgeStore::new(temp_dir.path().to_str().unwrap())
            .await
            .unwrap();

        // æ„é€ æ²¡æœ‰åˆ‡ç‰‡çš„ ParsedDocï¼ˆå›é€€é€»è¾‘ï¼‰
        let doc = ParsedDoc {
            path: "/legacy/doc.md".to_string(),
            title: "Legacy Doc".to_string(),
            summary: "Legacy summary".to_string(),
            content: "Full legacy content".to_string(),
            sections: vec![], // ç©ºåˆ‡ç‰‡ï¼Œè§¦å‘å›é€€é€»è¾‘
        };

        // è°ƒç”¨ add()
        let ids = store.add(&doc).await.unwrap();

        // æ–­è¨€ï¼šè¿”å› 1 ä¸ª IDï¼ˆæ•´ç¯‡æ–‡æ¡£ä½œä¸ºå•æ¡è®°å½•ï¼‰
        assert_eq!(ids.len(), 1);

        // æ–­è¨€ï¼šå­˜å‚¨ç›®å½•ä¸­æœ‰ 1 ä¸ª JSON æ–‡ä»¶
        let json_files: Vec<_> = fs::read_dir(temp_dir.path())
            .unwrap()
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("json"))
            .collect();
        assert_eq!(json_files.len(), 1);

        // æ–­è¨€ï¼šè®°å½•çš„ title æ˜¯æ–‡æ¡£æ ‡é¢˜ï¼ˆè€Œéåˆ‡ç‰‡æ ‡é¢˜ï¼‰
        let content = fs::read_to_string(json_files[0].path()).unwrap();
        let record: KnowledgeRecord = serde_json::from_str(&content).unwrap();
        assert_eq!(record.title, "Legacy Doc");
        assert_eq!(record.source_path, "/legacy/doc.md");
    }

    /// é²æ£’æ€§æµ‹è¯•ï¼ˆæç«¯æƒ…å†µï¼‰
    #[tokio::test]
    async fn test_storage_robustness() {
        let temp_dir = tempfile::tempdir().unwrap();
        let store = KnowledgeStore::new(temp_dir.path().to_str().unwrap())
            .await
            .unwrap();

        // æ„é€ æç«¯æ•°æ®
        let mut sections = vec![
            // Case A: æ ‡é¢˜ä¸ºç©ºï¼Œå†…å®¹åŒ…å« Emoji å’Œç‰¹æ®Šç¬¦å·
            SlicedSection {
                section_title: "".to_string(),
                content: "ğŸš€ Emoji & \"Quotes\" & \nNewlines".to_string(),
                parent_doc_title: "Edge Case Doc".to_string(),
                summary: "ğŸš€ Emoji & \"Quotes\" & \nNewlines".to_string(),
            },
            // Case B: åªæœ‰æ ‡é¢˜ï¼Œå†…å®¹ä¸ºç©º
            SlicedSection {
                section_title: "Empty Content".to_string(),
                content: "".to_string(),
                parent_doc_title: "Edge Case Doc".to_string(),
                summary: "".to_string(),
            },
        ];

        // Case C: å¤§é‡åˆ‡ç‰‡ (æ¨¡æ‹Ÿé•¿æ–‡) - å¾ªç¯ç”Ÿæˆ 50 ä¸ªåˆ‡ç‰‡
        for i in 0..50 {
            sections.push(SlicedSection {
                section_title: format!("Section {}", i),
                content: format!("Content for section {}", i),
                parent_doc_title: "Edge Case Doc".to_string(),
                summary: format!("Content for section {}", i),
            });
        }

        let doc = ParsedDoc {
            path: "C:\\Windows\\System32\\weird_path.md".to_string(), // Windows è·¯å¾„åæ–œæ æµ‹è¯•
            title: "Edge Case Doc".to_string(),
            summary: "".to_string(),
            content: "".to_string(),
            sections,
        };

        // éªŒè¯æ˜¯å¦èƒ½æˆåŠŸå†™å…¥ï¼Œä¸ Panic
        let ids = store.add(&doc).await.unwrap();

        // éªŒè¯ Case C: ç¡®ä¿ç”Ÿæˆçš„ ID æ•°é‡æ­£ç¡® (2ä¸ªæ‰‹åŠ¨ + 50ä¸ªå¾ªç¯ = 52)
        assert_eq!(ids.len(), 52);

        // éªŒè¯ JSON è¯»å–å›æ¥çš„æ•°æ®å®Œæ•´æ€§ (ç¡®ä¿ç‰¹æ®Šå­—ç¬¦æ²¡æœ‰ä¹±ç )
        // è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œååºåˆ—åŒ–ï¼Œæ–­è¨€ content == "ğŸš€ Emoji & \"Quotes\" & \nNewlines"
        let first_record = store.get(&ids[0]).await.unwrap().unwrap();
        assert_eq!(first_record.content, "ğŸš€ Emoji & \"Quotes\" & \nNewlines");
        assert_eq!(
            first_record.source_path,
            "C:\\Windows\\System32\\weird_path.md"
        );

        // éªŒè¯ Case B: ç©ºå†…å®¹åˆ‡ç‰‡ä¹Ÿèƒ½æ­£ç¡®å­˜å‚¨
        let second_record = store.get(&ids[1]).await.unwrap().unwrap();
        assert_eq!(second_record.title, "Empty Content");
        assert_eq!(second_record.content, "");

        // éªŒè¯ Case C: æ‰€æœ‰ ID éƒ½æ˜¯å”¯ä¸€çš„ï¼ˆé€šè¿‡é›†åˆå»é‡åæ•°é‡ä¸å˜ï¼‰
        let unique_ids: std::collections::HashSet<_> = ids.iter().collect();
        assert_eq!(unique_ids.len(), 52);
    }

    /// å›æ»šæœºåˆ¶æµ‹è¯•ï¼šéªŒè¯æˆåŠŸåä¸´æ—¶ç›®å½•è¢«æ¸…ç†
    #[tokio::test]
    async fn test_rollback_temp_cleanup() {
        let temp_dir = tempfile::tempdir().unwrap();
        let store = KnowledgeStore::new(temp_dir.path().to_str().unwrap())
            .await
            .unwrap();

        let doc = ParsedDoc {
            path: "/test/doc.md".to_string(),
            title: "Test Doc".to_string(),
            summary: "Test summary".to_string(),
            content: "Full content".to_string(),
            sections: vec![
                SlicedSection {
                    section_title: "Section 1".to_string(),
                    content: "Content 1".to_string(),
                    parent_doc_title: "Test Doc".to_string(),
                    summary: "Content 1".to_string(),
                },
                SlicedSection {
                    section_title: "Section 2".to_string(),
                    content: "Content 2".to_string(),
                    parent_doc_title: "Test Doc".to_string(),
                    summary: "Content 2".to_string(),
                },
            ],
        };

        // æ‰§è¡Œæ·»åŠ æ“ä½œï¼ˆåº”è¯¥æˆåŠŸï¼‰
        let ids = store.add(&doc).await.unwrap();
        assert_eq!(ids.len(), 2);

        // éªŒè¯ï¼šæ­£å¼ç›®å½•ä¸­æœ‰ 2 ä¸ª JSON æ–‡ä»¶
        let json_files: Vec<_> = fs::read_dir(temp_dir.path())
            .unwrap()
            .filter_map(|e| e.ok())
            .filter(|e| {
                e.path().extension().and_then(|s| s.to_str()) == Some("json")
                    && !e
                        .path()
                        .file_name()
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .starts_with(".temp-")
            })
            .collect();
        assert_eq!(json_files.len(), 2);

        // éªŒè¯ï¼šæ²¡æœ‰ä¸´æ—¶ç›®å½•æ®‹ç•™ï¼ˆæ‰€æœ‰ .temp-* ç›®å½•éƒ½è¢«æ¸…ç†ï¼‰
        let temp_dirs: Vec<_> = fs::read_dir(temp_dir.path())
            .unwrap()
            .filter_map(|e| e.ok())
            .filter(|e| {
                e.path()
                    .file_name()
                    .unwrap()
                    .to_str()
                    .unwrap()
                    .starts_with(".temp-")
            })
            .collect();
        assert_eq!(
            temp_dirs.len(),
            0,
            "Temporary directories should be cleaned up"
        );
    }

    /// åŸå­æ€§æµ‹è¯•ï¼šéªŒè¯æ‰€æœ‰æ–‡ä»¶è¦ä¹ˆéƒ½åœ¨ï¼Œè¦ä¹ˆéƒ½ä¸åœ¨
    #[tokio::test]
    async fn test_atomicity_all_or_nothing() {
        let temp_dir = tempfile::tempdir().unwrap();
        let store = KnowledgeStore::new(temp_dir.path().to_str().unwrap())
            .await
            .unwrap();

        // åˆ›å»ºåŒ…å« 5 ä¸ªåˆ‡ç‰‡çš„æ–‡æ¡£
        let mut sections = Vec::new();
        for i in 0..5 {
            sections.push(SlicedSection {
                section_title: format!("Section {}", i),
                content: format!("Content for section {}", i),
                parent_doc_title: "Atomicity Test".to_string(),
                summary: format!("Content for section {}", i),
            });
        }

        let doc = ParsedDoc {
            path: "/test/atomic.md".to_string(),
            title: "Atomicity Test".to_string(),
            summary: "Test".to_string(),
            content: "Full content".to_string(),
            sections,
        };

        // æ‰§è¡Œæ·»åŠ æ“ä½œ
        let ids = store.add(&doc).await.unwrap();

        // éªŒè¯ï¼šæ‰€æœ‰ 5 ä¸ªæ–‡ä»¶éƒ½å­˜åœ¨
        assert_eq!(ids.len(), 5);
        for id in &ids {
            let path = temp_dir.path().join(format!("{}.json", id));
            assert!(path.exists(), "File {} should exist", id);
        }

        // éªŒè¯ï¼šå¯ä»¥è¯»å–æ‰€æœ‰ 5 ä¸ªæ–‡ä»¶
        for id in &ids {
            let record = store.get(id).await.unwrap();
            assert!(record.is_some(), "Record {} should be retrievable", id);
        }
    }
}
