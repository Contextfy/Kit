# å®ç°ä»»åŠ¡æ¸…å•

## ä»»åŠ¡æ¦‚è§ˆ

æ ¹æ® Issue #3 çš„è¦æ±‚ï¼Œæœ¬æ¬¡å®ç°åˆ†ä¸ºä¸‰ä¸ªä¸»è¦ä»»åŠ¡ï¼š

- **Task-01**: æ•°æ®ç»“æ„æ›´æ–°ï¼ˆ`ParsedDoc` æ·»åŠ  `sections`ï¼Œ`KnowledgeRecord` æ·»åŠ  `source_path`ï¼‰
- **Task-02**: å­˜å‚¨é€»è¾‘å®ç°ï¼ˆé‡å†™ `add()` ä»¥æ‰å¹³åŒ–åˆ‡ç‰‡ä¸ºè®°å½•ï¼‰
- **Task-03**: éªŒè¯æµ‹è¯•ï¼ˆæ·»åŠ å•å…ƒæµ‹è¯•ç¡®ä¿åŠŸèƒ½æ­£ç¡®ï¼‰

---

## Task-01: ç»“æ„ä½“æ›´æ–°

**ä¸Šä¸‹æ–‡**:
- `packages/core/src/lib.rs` - å¯¼å‡ºå’Œç»“æ„ä½“å®šä¹‰
- `packages/core/src/storage/mod.rs` - å­˜å‚¨ç›¸å…³ç»“æ„ä½“
- `packages/core/src/parser/mod.rs` - è§£æé€»è¾‘
- `packages/cli/src/main.rs` - CLI æ¨¡å—ï¼ˆå¯èƒ½éœ€è¦ä¿®å¤ç¼–è¯‘é”™è¯¯ï¼‰

**å­ä»»åŠ¡**:
- [ ] 1.1 åœ¨ `packages/core/src/lib.rs` ä¸­å¯¼å…¥ `SlicedDoc`
  - æ·»åŠ  `pub use parser::{slice_by_headers, SlicedDoc};`
- [ ] 1.2 åœ¨ `ParsedDoc` ç»“æ„ä½“ä¸­æ·»åŠ  `pub sections: Vec<SlicedDoc>` å­—æ®µ
  - ä½ç½®ï¼š`packages/core/src/parser/mod.rs` æˆ– `lib.rs`ï¼ˆæ ¹æ®å½“å‰å®šä¹‰ä½ç½®ï¼‰
- [ ] 1.3 å¤„ç†ç”Ÿå‘½å‘¨æœŸé—®é¢˜
  - `SlicedDoc<'a>` æœ‰ç”Ÿå‘½å‘¨æœŸå‚æ•°ï¼Œéœ€è¦å†³å®š `ParsedDoc` çš„ç”Ÿå‘½å‘¨æœŸç­–ç•¥
  - **æ–¹æ¡ˆ A**ï¼šè®© `ParsedDoc` ä¹Ÿæºå¸¦ç”Ÿå‘½å‘¨æœŸ `ParsedDoc<'a>`
  - **æ–¹æ¡ˆ B**ï¼šå°† `SlicedDoc` æ”¹ä¸ºæ‹¥æœ‰æ‰€æœ‰æƒï¼ˆç§»é™¤ç”Ÿå‘½å‘¨æœŸï¼Œå¤åˆ¶æ•°æ®ï¼‰
  - **å»ºè®®**ï¼šå…ˆå°è¯•æ–¹æ¡ˆ Aï¼Œå¦‚æœ CLI æˆ–å­˜å‚¨å±‚ä½¿ç”¨è¿‡äºå¤æ‚ï¼Œå†è€ƒè™‘æ–¹æ¡ˆ B
- [ ] 1.4 åœ¨ `KnowledgeRecord` ç»“æ„ä½“ä¸­æ·»åŠ  `pub source_path: String` å­—æ®µ
  - ä½ç½®ï¼š`packages/core/src/storage/mod.rs`
- [ ] 1.5 ä¿®æ”¹æ‰€æœ‰ `KnowledgeRecord` åˆå§‹åŒ–ä»£ç 
  - æ£€æŸ¥ `storage/mod.rs` ä¸­çš„ `add()` æ–¹æ³•
  - ç¡®ä¿æ¯æ¬¡åˆ›å»º `KnowledgeRecord` æ—¶éƒ½ä¼ å…¥ `source_path`
- [ ] 1.6 ä¿®æ”¹ `parse_markdown()` å‡½æ•°ä»¥å¡«å…… `sections` å­—æ®µ
  - åœ¨è§£æå®Œæˆåè°ƒç”¨ `slice_by_headers(&content, &title)`
  - å°†ç»“æœèµ‹å€¼ç»™ `doc.sections`
- [ ] 1.7 æ£€æŸ¥å¹¶ä¿®å¤ CLI æ¨¡å—ä¸­çš„ç¼–è¯‘é”™è¯¯
  - è¿è¡Œ `cargo build --bin contextfy-cli`
  - ä¿®å¤ä»»ä½•å› ç»“æ„ä½“å­—æ®µå˜æ›´å¯¼è‡´çš„é”™è¯¯
- [ ] 1.8 è¿è¡Œ `cargo test -p contextfy-core` ç¡®ä¿æ²¡æœ‰ç ´åç°æœ‰æµ‹è¯•

**é¢„æœŸäº§å‡º**:
- `ParsedDoc` åŒ…å« `sections: Vec<SlicedDoc>` å­—æ®µ
- `KnowledgeRecord` åŒ…å« `source_path: String` å­—æ®µ
- æ‰€æœ‰æ¨¡å—ç¼–è¯‘é€šè¿‡ï¼Œæ— è­¦å‘Š

---

## Task-02: å­˜å‚¨é€»è¾‘å®ç°

**ä¸Šä¸‹æ–‡**:
- `packages/core/src/storage/mod.rs` - `KnowledgeStore::add()` æ–¹æ³•

**å­ä»»åŠ¡**:
- [ ] 2.1 åˆ†æå½“å‰ `add()` æ–¹æ³•çš„å®ç°é€»è¾‘
  - å½“å‰ï¼šåˆ›å»º 1 ä¸ª `KnowledgeRecord`ï¼Œåºåˆ—åŒ–ä¸º JSONï¼Œå†™å…¥æ–‡ä»¶
- [ ] 2.2 é‡å†™ `add()` æ–¹æ³•ä»¥æ”¯æŒåˆ‡ç‰‡å­˜å‚¨
  ```rust
  pub async fn add(&self, doc: &ParsedDoc) -> Result<Vec<String>> {
      let mut ids = Vec::new();

      if doc.sections.is_empty() {
          // å›é€€é€»è¾‘ï¼šå­˜å‚¨æ•´ä¸ªæ–‡æ¡£ä¸º 1 æ¡è®°å½•
          let id = Uuid::new_v4().to_string();
          let record = KnowledgeRecord {
              id: id.clone(),
              title: doc.title.clone(),
              summary: doc.summary.clone(),
              content: doc.content.clone(),
              source_path: doc.path.clone(),  // æ–°å¢å­—æ®µ
          };
          // åºåˆ—åŒ–å¹¶å†™å…¥æ–‡ä»¶...
          ids.push(id);
      } else {
          // æ–°é€»è¾‘ï¼šä¸ºæ¯ä¸ªåˆ‡ç‰‡åˆ›å»ºç‹¬ç«‹è®°å½•
          for slice in &doc.sections {
              let id = Uuid::new_v4().to_string();
              let record = KnowledgeRecord {
                  id: id.clone(),
                  title: slice.section_title.clone(),
                  summary: slice.content.chars().take(200).collect::<String>(),
                  content: slice.content.to_string(),  // æ³¨æ„ï¼šå¯èƒ½éœ€è¦å¤åˆ¶æ•°æ®
                  source_path: doc.path.clone(),
              };
              // åºåˆ—åŒ–å¹¶å†™å…¥æ–‡ä»¶...
              ids.push(id);
          }
      }

      Ok(ids)  // è¿”å›æ‰€æœ‰åˆ‡ç‰‡çš„ ID
  }
  ```
- [ ] 2.3 å¤„ç†ç”Ÿå‘½å‘¨æœŸå’Œæ•°æ®æ‰€æœ‰æƒé—®é¢˜
  - `SlicedDoc.content` æ˜¯ `&str`ï¼Œå­˜å‚¨æ—¶éœ€è¦è½¬æ¢ä¸º `String`
  - ä½¿ç”¨ `.to_string()` å¤åˆ¶æ•°æ®ï¼ˆæŸå¤±é›¶æ‹·è´ä¼˜åŠ¿ï¼Œä½† JSON åºåˆ—åŒ–ä¸å¯é¿å…ï¼‰
- [ ] 2.4 æ›´æ–°æ–¹æ³•ç­¾åè¿”å›ç±»å‹
  - ä» `Result<String>` æ”¹ä¸º `Result<Vec<String>>`ï¼ˆè¿”å›æ‰€æœ‰åˆ‡ç‰‡çš„ IDï¼‰
- [ ] 2.5 ä¿®å¤è°ƒç”¨ç‚¹
  - CLI æ¨¡å—ä¸­çš„ `store.add(&doc).await?` éœ€è¦é€‚é…æ–°çš„è¿”å›ç±»å‹
- [ ] 2.6 æ·»åŠ é”™è¯¯å¤„ç†
  - å¤„ç†ç©ºåˆ‡ç‰‡ã€ç©ºå†…å®¹ç­‰è¾¹ç•Œæƒ…å†µ
- [ ] 2.7 ç¼–å†™ä¸´æ—¶è°ƒè¯•æ—¥å¿—
  - æ‰“å°å­˜å‚¨çš„åˆ‡ç‰‡æ•°é‡å’Œ ID åˆ—è¡¨

**é¢„æœŸäº§å‡º**:
- `add()` æ–¹æ³•èƒ½å°† `ParsedDoc.sections` æ‰å¹³åŒ–ä¸ºå¤šæ¡è®°å½•
- æ¯æ¡è®°å½•åŒ…å« `source_path` å­—æ®µ
- è¿”å›æ‰€æœ‰åˆ‡ç‰‡çš„ UUID åˆ—è¡¨

---

## Task-03: éªŒè¯æµ‹è¯•

**ä¸Šä¸‹æ–‡**:
- `packages/core/src/storage/mod.rs` - å•å…ƒæµ‹è¯•

**å­ä»»åŠ¡**:
- [ ] 3.1 ç¼–å†™å•å…ƒæµ‹è¯• `test_add_sliced_doc`
  ```rust
  #[tokio::test]
  async fn test_add_sliced_doc() {
      // åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
      let temp_dir = tempfile::tempdir().unwrap();
      let store = KnowledgeStore::new(temp_dir.path().to_str().unwrap()).unwrap();

      // æ‰‹åŠ¨æ„é€ åŒ…å« 2 ä¸ªåˆ‡ç‰‡çš„ ParsedDoc
      let doc = ParsedDoc {
          path: "/fake/path.md".to_string(),
          title: "Test Doc".to_string(),
          summary: "Test summary".to_string(),
          content: "Full content".to_string(),
          sections: vec![
              SlicedDoc {
                  section_title: "Section 1".to_string(),
                  content: "Content 1",
                  parent_doc_title: "Test Doc",
              },
              SlicedDoc {
                  section_title: "Section 2".to_string(),
                  content: "Content 2",
                  parent_doc_title: "Test Doc",
              },
          ],
      };

      // è°ƒç”¨ add()
      let ids = store.add(&doc).await.unwrap();

      // æ–­è¨€ï¼šè¿”å› 2 ä¸ª ID
      assert_eq!(ids.len(), 2);

      // æ–­è¨€ï¼šå­˜å‚¨ç›®å½•ä¸­æœ‰ 2 ä¸ª JSON æ–‡ä»¶
      let json_files: Vec<_> = fs::read_dir(temp_dir.path())
          .unwrap()
          .filter_map(|e| e.ok())
          .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("json"))
          .collect();
      assert_eq!(json_files.len(), 2);

      // æ–­è¨€ï¼šæ¯ä¸ªè®°å½•éƒ½æœ‰æ­£ç¡®çš„ source_path
      for json_file in json_files {
          let content = fs::read_to_string(json_file.path()).unwrap();
          let record: KnowledgeRecord = serde_json::from_str(&content).unwrap();
          assert_eq!(record.source_path, "/fake/path.md");
      }
  }
  ```
- [ ] 3.2 ç¼–å†™è¾¹ç•Œæƒ…å†µæµ‹è¯• `test_add_empty_sections`
  - æµ‹è¯• `sections` ä¸ºç©ºæ—¶çš„å›é€€é€»è¾‘
  - ç¡®ä¿æ•´ä¸ªæ–‡æ¡£ä½œä¸º 1 æ¡è®°å½•å­˜å‚¨
- [ ] 3.3 ç¼–å†™é²æ£’æ€§æµ‹è¯• `test_storage_robustness` (æç«¯æƒ…å†µ)
  ```rust
  #[tokio::test]
  async fn test_storage_robustness() {
      let temp_dir = tempfile::tempdir().unwrap();
      let store = KnowledgeStore::new(temp_dir.path().to_str().unwrap()).unwrap();

      // æ„é€ æç«¯æ•°æ®
      let mut sections = vec![
          // Case A: æ ‡é¢˜ä¸ºç©ºï¼Œå†…å®¹åŒ…å« Emoji å’Œç‰¹æ®Šç¬¦å·
          SlicedDoc {
              section_title: "".to_string(),
              content: "ğŸš€ Emoji & \"Quotes\" & \nNewlines".to_string(),
              parent_doc_title: "Edge Case Doc",
          },
          // Case B: åªæœ‰æ ‡é¢˜ï¼Œå†…å®¹ä¸ºç©º
          SlicedDoc {
              section_title: "Empty Content".to_string(),
              content: "".to_string(),
              parent_doc_title: "Edge Case Doc",
          },
      ];

      // Case C: å¤§é‡åˆ‡ç‰‡ (æ¨¡æ‹Ÿé•¿æ–‡) - å¾ªç¯ç”Ÿæˆ 50 ä¸ªåˆ‡ç‰‡
      for i in 0..50 {
          sections.push(SlicedDoc {
              section_title: format!("Section {}", i),
              content: format!("Content for section {}", i),
              parent_doc_title: "Edge Case Doc",
          });
      }

      let doc = ParsedDoc {
          path: "C:\\Windows\\System32\\weird_path.md".to_string(), // Windows è·¯å¾„åæ–œæ æµ‹è¯•
          title: "Edge Case Doc".to_string(),
          summary: "".to_string(),
          content: "".to_string(),
          sections,
      };

      // éªŒè¯æ˜¯å¦èƒ½æˆåŠŸå†™å…¥ï¼Œä¸ Panic
      let ids = store.add(&doc).await.unwrap();

      // éªŒè¯ Case C: ç¡®ä¿ç”Ÿæˆçš„ ID æ•°é‡æ­£ç¡® (2ä¸ªæ‰‹åŠ¨ + 50ä¸ªå¾ªç¯ = 52)
      assert_eq!(ids.len(), 52);

      // éªŒè¯ JSON è¯»å–å›æ¥çš„æ•°æ®å®Œæ•´æ€§ (ç¡®ä¿ç‰¹æ®Šå­—ç¬¦æ²¡æœ‰ä¹±ç )
      // è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œååºåˆ—åŒ–ï¼Œæ–­è¨€ content == "ğŸš€ Emoji & \"Quotes\" & \nNewlines"
      let first_record = store.get(&ids[0]).await.unwrap().unwrap();
      assert_eq!(first_record.content, "ğŸš€ Emoji & \"Quotes\" & \nNewlines");
      assert_eq!(first_record.source_path, "C:\\Windows\\System32\\weird_path.md");
  }
  ```
- [ ] 3.4 ç¼–å†™ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
  - ä½¿ç”¨çœŸå®çš„ markdown æ–‡ä»¶
  - è°ƒç”¨ `parse_markdown()` â†’ `store.add()` â†’ éªŒè¯å­˜å‚¨ç»“æœ
- [ ] 3.5 è¿è¡Œæ‰€æœ‰æµ‹è¯•å¹¶ç¡®ä¿é€šè¿‡
  ```bash
  cargo test -p contextfy-core
  ```
- [ ] 3.6 è¿è¡Œä»£ç æ ¼å¼åŒ–å’Œé™æ€æ£€æŸ¥
  ```bash
  cargo fmt
  cargo clippy -p contextfy-core
  ```
- [ ] 3.7 æ‰‹åŠ¨æµ‹è¯• CLI æµç¨‹
  ```bash
  cd /home/haotang/my-project/contextfy/Kit
  cargo build --bin contextfy-cli
  # åˆ›å»ºæµ‹è¯•æ–‡æ¡£å¹¶è¿è¡Œ contextfy build
  # æ£€æŸ¥ .contextfy/data/ ç›®å½•ä¸­çš„ JSON æ–‡ä»¶æ•°é‡
  ```

**é¢„æœŸäº§å‡º**:
- å•å…ƒæµ‹è¯•è¦†ç›–ä¸»è¦è·¯å¾„å’Œè¾¹ç•Œæƒ…å†µ
- æ‰€æœ‰æµ‹è¯•é€šè¿‡
- ä»£ç é€šè¿‡ fmt å’Œ clippy æ£€æŸ¥

---

## å®ç°äº®ç‚¹è®°å½•

å®Œæˆæ‰€æœ‰ä»»åŠ¡åï¼Œåœ¨æ­¤å¤„è®°å½•å®ç°äº®ç‚¹å’ŒæŠ€æœ¯å†³ç­–ï¼š

_ï¼ˆç•™å¾…å®ç°å®Œæˆåå¡«å†™ï¼‰_

### è®¾è®¡å†³ç­–
- _ï¼ˆè®°å½•å…³é”®æŠ€æœ¯é€‰æ‹©ï¼Œå¦‚ç”Ÿå‘½å‘¨æœŸå¤„ç†ã€é”™è¯¯å¤„ç†ç­–ç•¥ç­‰ï¼‰_

### å·²çŸ¥é—®é¢˜
- _ï¼ˆè®°å½•ä»»ä½•é—ç•™é—®é¢˜æˆ–é™åˆ¶ï¼‰_

### åç»­ä¼˜åŒ–æ–¹å‘
- _ï¼ˆè®°å½•æœªæ¥å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ï¼‰_
